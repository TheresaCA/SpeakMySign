heading
SpeakMySign: An ISL-to-Speech Translator Using Machine Learning and Large Language Models

Theresa Clare Alex
Department of Computer Science Engineering
PES University 
Bengaluru, India
theresacalex@gmail.com
 
Tejaswini V
Department of Computer Science Engineering
PES University 
Bengaluru, India
line 5: email address or ORCID

abstract
Abstract— Communication is crucial in our day-to-day lives. It is the basis for all human interactions. The communication gap between the hearing and speech-impaired populations especially is a very large and extremely populated country like India continues to be a major obstacle. Such a person would use sign language to communicate, but sign language is not known by everyone. To bridge this communication gap a translation system is required and SpeakMySign offers to do the same. SpeakMySign is an end-to-end translation system that converts Indian Sign Language (ISL) to English, Hindi and Kannada speech based on a mix of computer vision, deep learning, and language models.

Keywords— Indian Sign Language (ISL), Sign Language Translation, Speech Synthesis, Computer Vision, Deep Learning, Human-Computer Interaction, Multilingual Communication, Accessibility Technology.

introduction
In recent years, the global community has made strides in promoting inclusivity for individuals with disabilities, particularly those with hearing and speech impairments. Indian Sign Language (ISL) serves as the primary mode of communication for millions of people in India, yet the lack of effective communication between ISL users and non-ISL speakers remains a significant challenge. The barriers in communication not only impede social interaction but also hinder access to essential services such as education, healthcare, and employment.
This paper proposes the development of an AI-powered system aimed at bridging the communication gap between ISL users and non-ISL speakers. The system leverages advanced computer vision models to recognize ISL gestures, which are then translated into text using fine-tuned large language models (LLMs) that are specifically designed to understand the grammar and syntax of ISL. To enhance the accuracy and contextual understanding of the translation, we integrate Retrieval-Augmented Generation (RAG), a technique that improves the quality of responses by leveraging external information, thus refining the sentence structure and capturing the nuances of sign language.
The output text is then converted into natural-sounding speech through the use of text-to-speech (TTS) models, facilitating real-time communication for ISL users in various contexts such as education, healthcare, workplaces, and public services. This innovative system aims to offer a more adaptive and context-aware translation solution compared to traditional ISL translation systems, ensuring that diverse scenarios are accurately interpreted and conveyed.
By incorporating state-of-the-art AI techniques, this project aims to empower individuals with hearing impairments, providing them with an accessible and inclusive tool for seamless communication with the broader society. The successful implementation of this system promises to enhance the quality of life for ISL users, promoting greater inclusivity and breaking down communication barriers in a variety of settings.

literature survey
Paper 1: A Framework for Sign Language to Speech Conversion Using Hand Gesture Recognition Method
This paper presents a gesture recognition framework aimed at bridging communication gaps between speech-impaired individuals and the hearing community. The proposed system uses computer vision techniques to recognize Indian Sign Language (ISL) gestures and convert them into text and speech. A CNN-based model is used for classifying 26 hand gestures, enabling real-time interaction without requiring an interpreter. The system improves accessibility and inclusivity for disabled individuals while also being applicable in various technical domains.

Paper 2: Real-Time Sign Language to Text and Speech Translation and Hand Gesture Recognition using the LSTM Model
This work introduces a real-time system that translates sign language gestures into text and speech using LSTM models. The system uses Mediapipe for hand tracking and gesture extraction, converting the recognized gestures into sentences and synthesizing speech output. The research emphasizes the integration of LSTM and CNN for high accuracy and aims to facilitate two-way communication between the speech/hearing impaired and the general population.

Paper 3: Indian Sign Language Translator
This paper proposes a bilingual system for converting Indian Sign Language (ISL) into speech and vice versa using animation and gesture recognition. The system includes a dataset of gesture videos and animated ISL gestures, targeting medical and greeting-related vocabulary. It combines Mediapipe-based pre-processing with gesture classification, supporting ISL-to-speech and speech-to-ISL translation, and helps enhance communication accessibility in public and healthcare settings.

Paper 4: LLMs are Good Sign Language Translators
This paper introduces SignLLM, a novel framework that enables off-the-shelf large language models (LLMs) to translate sign language videos into spoken language. It achieves this by converting videos into discrete, hierarchical language-like sign tokens using a Vector-Quantized module and a Codebook Alignment process. Without gloss annotations, the method achieves state-of-the-art results on benchmark datasets, showcasing the feasibility of leveraging LLMs in low-resource sign language translation tasks.

Paper 5: Leveraging Large Language Models With Vocabulary Sharing For Sign Language Translation
This study investigates the application of large language models (LLMs) with vocabulary sharing for gloss-based sign language translation. Focusing on Korean Sign Language, it explores text-to-gloss and gloss-to-text tasks using a GPT-3-based model trained on a large parallel dataset. The proposed method outperforms baseline models, demonstrating how LLMs can mitigate the data scarcity problem and improve translation accuracy through lexical alignment.

Paper 6: Sign Language to Text Translation with Computer Vision: Bridging the Communication Gap
This research develops a real-time sign language translation system using computer vision and deep learning techniques. Static and dynamic signs are recognized via CNN and LSTM, respectively, with an integrated LLM for sentence generation. The system achieves high accuracy (99.2% for static, 90.08% for dynamic), and includes text-to-speech output. Though real-world variability introduces some errors, the system effectively enhances accessibility and inclusivity for the hearing impaired.


